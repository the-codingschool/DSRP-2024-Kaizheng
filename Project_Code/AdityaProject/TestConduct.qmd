```{r}
library(readr)
library(dplyr)
library(janitor)
library(ggplot2)
library(caTools)
library(downloader)
library(FNN)
library(readxl)
library(shapr)
library(caret)
library(randomForest)


```

```{r}
traffic <- read.csv("Project_Code/Data/data_traffic.csv")

traffic

#acc_info <- select(traffic, c(unique_id, case_id_pkey, tb_latitude, tb_longitude, collision_time, day_of_week, time_cat, reporting_district, primary_rd, secondary_rd, distance, collision_severity, type_of_collision, mviw, ped_action, road_surface, lighting, intersection, vz_pcf_description, number_killed, number_injured, at_fault, dph_col_grp_description))







```

```{r}
people_demo_categ <- people_demo

people_demo_categ


#Significance of gender
con_tab_sex <- table(people_demo_categ$party_sex, people_demo_categ$at_fault)
con_tab_sex

chisq_sex <- chisq.test(con_tab_sex)
chisq_sex

#3.914e-09


#Significance of sobriety
con_tab_sob <- table(people_demo_categ$party_sobriety, people_demo_categ$at_fault)
con_tab_sob

chisq_sob <- chisq.test(con_tab_sob)
chisq_sob

#< 2.2e-16

#Significance of race
con_tab_race <- table(people_demo_categ$race, people_demo_categ$at_fault)
#con_tab_race 

chisq_race <- chisq.test(con_tab_race)
chisq_race

#4.085e-16


#Significance of driver type
con_tab_type <- table(people_demo_categ$party_type, people_demo_categ$at_fault)
#con_tab_type

chisq_type <- chisq.test(con_tab_type)
chisq_type

#< 2.2e-16

```

```{r}
#Modeling based on demographics

people_demographics <- people_demo_changed

people_demographics

set.seed(9000)

split_dem <- sample.split(people_demographics$at_fault, SplitRatio = 0.8)
train_dem <- subset(people_demographics, split_dem == T)
test_dem <- subset(people_demographics, split_dem == F)

normalize <- function(x) {
  return ( (x - min(x)) / (max(x) - min(x)))
}

train_dem_standard <- as.data.frame(lapply(train_dem[, c(1,3,4,5,6,7,8,9,10,11,12,13,14)], normalize))
test_dem_standard <- as.data.frame(lapply(test_dem[, c(1,3,4,5,6,7,8,9,10,11,12,13,14)], normalize))

train_dem_standard

train_dem
str(test_dem_standard)

#Usually we standardize the data before to reduce all the features to the same scale, but does it need to be done in this case and if so, should we also scale after one hot encoding

#scaled_train_dem <- scale(train_dem, center = TRUE, scale = TRUE)
#scaled_test_dem <- scale(test_dem, center = TRUE, scale = TRUE)

#scaled_train_dem
#scaled_test_dem



#Determining the k


?n

str(test_dem_standard)


k_num <- as.integer(sqrt(nrow(test_dem_standard)))

k_num





dem_model <- knn(train = train_dem_standard, test = test_dem_standard, cl = train_dem$at_fault, k = k_num)

dem_model

actual_vals <- factor(test_dem$at_fault)

actual_vals

f1_score <- confusionMatrix(factor(dem_model), actual_vals, mode = "everything", positive = "1")

f1_score



overall_confusion_matrix <- table(test_dem$at_fault, dem_model)
overall_confusion_matrix

overall_accuracy <- sum(diag(overall_confusion_matrix))/sum(overall_confusion_matrix)
overall_accuracy

#99.87%

#Testing based on race


'
white_test <- filter(test_dem, race == 4)
white_test

white_dem_model <- knn(train = train_dem, test = white_test, cl = train_dem$at_fault, k = 5)


white_confusion_matrix <- table(white_test$at_fault, white_dem_model)
white_confusion_matrix

white_accuracy <- sum(diag(white_confusion_matrix))/sum(white_confusion_matrix)
white_accuracy

98.25% accuracy
'

'
asian_test <- filter(test_dem, race == 1)
asian_test

asian_dem_model <- knn(train = train_dem, test = asian_test, cl = train_dem$at_fault, k = 5)


asian_confusion_matrix <- table(asian_test$at_fault, asian_dem_model)
asian_confusion_matrix

asian_accuracy <- sum(diag(asian_confusion_matrix))/sum(asian_confusion_matrix)
asian_accuracy

96.23%
'

'
black_test <- filter(test_dem, race == 2)
black_test

black_dem_model <- knn(train = train_dem, test = black_test, cl = train_dem$at_fault, k = 5)


black_confusion_matrix <- table(black_test$at_fault, black_dem_model)
black_confusion_matrix

black_accuracy <- sum(diag(black_confusion_matrix))/sum(black_confusion_matrix)
black_accuracy

96.51%
'

'
hispanic_test <- filter(test_dem, race == 3)
hispanic_test

hispanic_dem_model <- knn(train = train_dem, test = hispanic_test, cl = train_dem$at_fault, k = 5)


hispanic_confusion_matrix <- table(hispanic_test$at_fault, hispanic_dem_model)
hispanic_confusion_matrix

hispanic_accuracy <- sum(diag(hispanic_confusion_matrix))/sum(hispanic_confusion_matrix)
hispanic_accuracy

98.77%
'


```




```{r}
accident_info <- select(traffic, c(day_of_week, time_cat, primary_rd, secondary_rd, distance, collision_severity, type_of_collision, ped_action, road_surface, lighting, number_killed, number_injured, party_type, party_sex, party_age, party_sobriety, race, accident_year, month, weather_1, vz_pcf_description, stwd_vehicle_type, vehicle_autonomous, at_fault))

accident_info

count(accident_info, vz_pcf_description)

ncol(accident_info)


CHIS <- lapply(accident_info[,-24], function(x) chisq.test(table(accident_info[,24], factor(x))))

do.call(rbind, CHIS) [, c(1,3)]



```

```{r}
standardized_accident_info <- sig_accident_info

standardized_accident_info <- as.data.frame(lapply(sig_accident_info[,], normalize))

standardized_accident_info



set.seed(135)


standard_split <- sample.split(standardized_accident_info$at_fault, SplitRatio = 0.8)

train_stand <- subset(standardized_accident_info, standard_split == T)
test_stand <- subset(standardized_accident_info, standard_split == F)


#Usually we standardize the data before to reduce all the features to the same scale, but does it need to be done in this case and if so, should we also scale after one hot encoding

#scaled_train_dem <- scale(train_dem, center = TRUE, scale = TRUE)
#scaled_test_dem <- scale(test_dem, center = TRUE, scale = TRUE)

#scaled_train_dem
#scaled_test_dem



#Determining the k

k_num_stand <- as.integer(sqrt(nrow(test_stand)))
k_num_stand



stand_model <- knn(train = train_stand, test = test_stand, cl = train_stand$at_fault, k = k_num_stand)

stand_model


actual_stand_vals <- factor(test_stand$at_fault)
actual_stand_vals

f1_score_stand <- confusionMatrix(factor(stand_model), actual_stand_vals, mode = "everything", positive = "1")

f1_score_stand



standard_confusion_matrix <- table(test_stand$at_fault, stand_model)
standard_confusion_matrix

standard_accuracy <- sum(diag(standard_confusion_matrix))/sum(standard_confusion_matrix)
standard_accuracy

```

